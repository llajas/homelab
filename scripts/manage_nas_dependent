#!/bin/bash
# Enable strict error handling: exit on error, treat unset variables as errors, and catch errors in pipelines
set -euo pipefail

# Function to ensure required commands are available
check_dependencies() {
  for cmd in kubectl argocd jq; do
    if ! command -v "$cmd" &>/dev/null; then
      echo "Error: Required command '$cmd' is not installed or not in PATH." >&2
      exit 1
    fi
  done
}

# Function to check if NAS address is provided
check_nas_address() {
  if [[ -z "${NAS_ADDRESS:-}" ]]; then
    echo "Error: Please set the NAS_ADDRESS environment variable." >&2
    exit 1
  fi
}

# Function to check if ArgoCD credentials are provided
check_argocd_env_vars() {
  if [[ -z "${ARGOCD_SERVER:-}" || -z "${ARGOCD_USERNAME:-}" || -z "${ARGOCD_PASSWORD:-}" ]]; then
    echo "Error: Please set ARGOCD_SERVER, ARGOCD_USERNAME, and ARGOCD_PASSWORD environment variables." >&2
    exit 1
  fi
}

# Function to normalize NAS_ADDRESS into SMB and NFS formats for comparison
normalize_nas_address() {
  # SMB format: ensure it starts with '//' and has no trailing slash
  if [[ "$NAS_ADDRESS" != //* ]]; then
    SMB_NAS_ADDRESS="//$NAS_ADDRESS"
  else
    SMB_NAS_ADDRESS="$NAS_ADDRESS"
  fi
  SMB_NAS_ADDRESS="${SMB_NAS_ADDRESS%/}"  # remove trailing slash if any

  # NFS format: ensure no leading '//' and no trailing slash
  NFS_NAS_ADDRESS="${NAS_ADDRESS#//}"
  NFS_NAS_ADDRESS="${NFS_NAS_ADDRESS%/}"
}

# Function to get list of Persistent Volumes (PVs) that use the NAS address
get_pv_list_dependent_on_nas() {
  normalize_nas_address  # prepare SMB_NAS_ADDRESS and NFS_NAS_ADDRESS variables
  # Query all PVs and filter those whose NFS server or CSI source matches the NAS addresses
  pv_list=$(kubectl get pv -o json | jq -r --arg SMB "$SMB_NAS_ADDRESS" --arg NFS "$NFS_NAS_ADDRESS" '
    .items[]
    | select(
        # Match if PV is CSI (likely SMB) and source equals the SMB NAS address (with or without trailing slash)
        (.spec.csi.volumeAttributes.source != null and
         (.spec.csi.volumeAttributes.source == $SMB or .spec.csi.volumeAttributes.source == ($SMB + "/")))
        # Or match if PV is NFS and server equals the NFS NAS address
        or (.spec.nfs.server != null and .spec.nfs.server == $NFS)
      )
    | .metadata.name
  ')
}

# Function to get unique namespaces for the affected PVs (those using the NAS)
get_namespaces_for_pvs() {
  if [[ -z "$pv_list" ]]; then
    echo "No persistent volumes found for NAS address '$NAS_ADDRESS'. Nothing to do."
    namespaces=""  # no namespaces to process
    return
  fi
  # Prepare PV list as JSON array for jq filtering
  local jq_pv_list
  jq_pv_list=$(echo "$pv_list" | jq -R -s -c 'split("\n") | map(select(. != ""))')
  # Find all namespaces of PVCs that bind to the PVs in pv_list
  namespaces=$(kubectl get pv -o json | jq -r --argjson PV_LIST "$jq_pv_list" '
    .items[]
    | select(.metadata.name as $name | ($PV_LIST | index($name)) )
    | .spec.claimRef.namespace
  ' | sort -u)
}

# Function to log in to ArgoCD (only if not already logged in)
argocd_login() {
  # Use ArgoCD CLI to check login status; if not logged in, perform login
  if argocd account get-user-info 2>&1 | grep -q 'Logged In: false'; then
    echo "Logging into ArgoCD CLI..."
    argocd login "$ARGOCD_SERVER" --username "$ARGOCD_USERNAME" --password "$ARGOCD_PASSWORD" --grpc-web
  fi
}

# Function to create or update an ArgoCD sync window that blocks auto-sync for affected apps
create_sync_window() {
  echo "Configuring ArgoCD sync window to block auto-sync for NAS-affected workloads..."
  local schedule="* * * * *"    # every minute (continuous window)
  local duration="24h"          # duration of each window occurrence
  local timeZone="America/Chicago"  # our preferred time zone

  # Combine all affected namespaces into a comma-separated list
  local affected_namespaces
  affected_namespaces=$(echo "$namespaces" | tr '\n' ',' | sed 's/,$//')

  # List existing sync windows in the ArgoCD project (assumed "default" project)
  local existing_window_json
  existing_window_json=$(argocd proj windows list default -o json || echo "[]")

  # Match based solely on schedule, duration, and kind (ignoring timeZone)
  local matching_window
  matching_window=$(echo "$existing_window_json" | jq -r --arg sched "$schedule" --arg dur "$duration" '
    .[]
    | select(.schedule == $sched and .duration == $dur and .kind == "deny")
  ')

  if [[ -n "$matching_window" ]]; then
    # A matching window was found; update its namespace list.
    local current_namespaces window_id updated_namespaces
    current_namespaces=$(echo "$matching_window" | jq -r '.namespaces | join(",")')
    window_id=$(echo "$existing_window_json" | jq -r --arg sched "$schedule" --arg dur "$duration" '
      to_entries[]
      | select(.value.schedule == $sched and .value.duration == $dur and .value.kind == "deny")
      | .key
    ')
    updated_namespaces=$(echo "$current_namespaces,$affected_namespaces" | tr ',' '\n' | sed '/^$/d' | sort -u | paste -sd, -)
    echo "Existing sync window found (ID=$window_id). Updating namespaces list to: $updated_namespaces"
    argocd proj windows update default "$window_id" --namespaces "$updated_namespaces" --time-zone "$timeZone"
  else
    echo "No existing sync window with given schedule/duration. Creating a new deny sync window for namespaces: $affected_namespaces"
    argocd proj windows add default \
      --kind deny \
      --schedule "$schedule" \
      --duration "$duration" \
      --namespaces "$affected_namespaces" \
      --time-zone "$timeZone" \
      --manual-sync
  fi
}

# Function to identify all deployments, statefulsets, and daemonsets that use the affected PVCs
get_dependent_workloads() {
  deployments=()
  statefulsets=()
  daemonsets=()
  pvc_list_arr=()  # array for PVC references (namespace/name)

  # Build list of PVCs (namespace/name) bound to each PV in pv_list
  # This loops through each PV name and gets its claimRef (namespace and claim name)
  for pv in $pv_list; do
    # Get the PVC name and namespace for this PV
    local claim namespace
    claim=$(kubectl get pv "$pv" -o jsonpath='{.spec.claimRef.name}')
    namespace=$(kubectl get pv "$pv" -o jsonpath='{.spec.claimRef.namespace}')
    if [[ -n "$claim" && -n "$namespace" ]]; then
      pvc_list_arr+=("${namespace}/${claim}")
    fi
  done

  if [[ ${#pvc_list_arr[@]} -eq 0 ]]; then
    echo "No PersistentVolumeClaims found for NAS PVs. Exiting."
    return
  fi

  echo "Identifying workloads (Deployments, StatefulSets, DaemonSets) using PVCs on NAS..."
  # For each PVC, find any Deployments/StatefulSets/DaemonSets that reference it in their volumes
  for pvc in "${pvc_list_arr[@]}"; do
    local ns pvc_name
    ns="${pvc%%/*}"       # namespace
    pvc_name="${pvc##*/}" # claim name

    # Check deployments in this namespace for volume claims matching pvc_name
    local deploys
    deploys=$(kubectl get deployments -n "$ns" -o json | jq -r --arg claim "$pvc_name" \
      '.items[] | select(.spec.template.spec.volumes[]? | .persistentVolumeClaim? and .persistentVolumeClaim.claimName == $claim) | .metadata.name')
    for d in $deploys; do
      # Avoid duplicates
      if [[ -n "$d" && ! " ${deployments[*]} " =~ " $ns/$d " ]]; then
        deployments+=("$ns/$d")
      fi
    done

    # Check statefulsets in this namespace
    local stss
    stss=$(kubectl get statefulsets -n "$ns" -o json | jq -r --arg claim "$pvc_name" \
      '.items[] | select(.spec.template.spec.volumes[]? | .persistentVolumeClaim? and .persistentVolumeClaim.claimName == $claim) | .metadata.name')
    for s in $stss; do
      if [[ -n "$s" && ! " ${statefulsets[*]} " =~ " $ns/$s " ]]; then
        statefulsets+=("$ns/$s")
      fi
    done

    # Check daemonsets in this namespace
    local dss
    dss=$(kubectl get daemonsets -n "$ns" -o json | jq -r --arg claim "$pvc_name" \
      '.items[] | select(.spec.template.spec.volumes[]? | .persistentVolumeClaim? and .persistentVolumeClaim.claimName == $claim) | .metadata.name')
    for ds in $dss; do
      if [[ -n "$ds" && ! " ${daemonsets[*]} " =~ " $ns/$ds " ]]; then
        daemonsets+=("$ns/$ds")
      fi
    done
  done

  # Output the list of affected workloads
  echo "Deployments using NAS PVCs: ${deployments[*]:-(none)}"
  echo "StatefulSets using NAS PVCs: ${statefulsets[*]:-(none)}"
  echo "DaemonSets using NAS PVCs: ${daemonsets[*]:-(none)}"
}

# Function to scale down (zero replicas or unschedule) the identified workloads
scale_down_workloads() {
  echo "Scaling down identified workloads (setting replicas to 0 or unscheduling pods)..."
  # Scale down deployments
  for deployment in "${deployments[@]}"; do
    local ns name
    ns="${deployment%%/*}"
    name="${deployment##*/}"
    echo " - Scaling down Deployment $name in namespace $ns"
    kubectl label deployment "$name" -n "$ns" nas-dependent=scaled --overwrite
    kubectl scale deployment "$name" -n "$ns" --replicas=0
  done

  # Scale down statefulsets
  for statefulset in "${statefulsets[@]}"; do
    local ns name
    ns="${statefulset%%/*}"
    name="${statefulset##*/}"
    echo " - Scaling down StatefulSet $name in namespace $ns"
    kubectl label statefulset "$name" -n "$ns" nas-dependent=scaled --overwrite
    kubectl scale statefulset "$name" -n "$ns" --replicas=0
  done

  # "Scale down" daemonsets by preventing scheduling on any nodes
  for daemonset in "${daemonsets[@]}"; do
    local ns name
    ns="${daemonset%%/*}"
    name="${daemonset##*/}"
    echo " - Pausing DaemonSet $name in namespace $ns (unscheduling its pods)"
    kubectl label daemonset "$name" -n "$ns" nas-dependent=scaled --overwrite
    # Patch the DaemonSet's pod template with an extra nodeSelector that matches no node (to remove all pods)
    kubectl patch daemonset "$name" -n "$ns" --type=merge -p '{"spec": {"template": {"spec": {"nodeSelector": {"nas-dependent-paused": "true"}}}}}'
    # Note: This effectively removes DaemonSet pods. The label 'nas-dependent-paused=true' is assumed not present on any node.
  done

  echo "All targeted workloads have been scaled down or paused."
}

# Function to remove/adjust the ArgoCD sync window and restore workloads (scale up)
# This is used when bringing the system back "up".
revert_sync_window() {
  echo "Reverting sync window and restoring scaled-down workloads via ArgoCD..."
  # Identify namespaces that were scaled down (marked by nas-dependent=scaled label on workloads)
  local scaled_down_namespaces
  scaled_down_namespaces=$(kubectl get deployment,statefulset,daemonset -A -l nas-dependent=scaled -o json |
    jq -r '.items[].metadata.namespace' | sort -u)

  # Use the same schedule/duration/timeZone as in create_sync_window to find the window
  local schedule="* * * * *"
  local duration="24h"
  local timeZone="America/Chicago"

  # Get the existing window (if any) matching those parameters
  local existing_window_json matching_window window_id
  existing_window_json=$(argocd proj windows list default -o json || echo "[]")
  matching_window=$(echo "$existing_window_json" | jq -r --arg sched "$schedule" --arg dur "$duration" --arg tz "$timeZone" '
    .[] | select(.schedule == $sched and .duration == $dur and (.timeZone // "" ) == $tz and .kind == "deny")
  ')
  if [[ -n "$matching_window" ]]; then
    # Find window ID (index in the list output)
    window_id=$(echo "$existing_window_json" | jq -r --arg sched "$schedule" --arg dur "$duration" --arg tz "$timeZone" '
      to_entries[] | select(.value.schedule == $sched and .value.duration == $dur and (.value.timeZone // "" ) == $tz and .value.kind == "deny") | .key
    ')
    # Compute remaining namespaces after removing the scaled-down ones from this window
    local current_namespaces remaining_namespaces
    current_namespaces=$(echo "$matching_window" | jq -r '.namespaces | join(",")')
    remaining_namespaces=$(comm -23 \
      <(echo "$current_namespaces" | tr ',' '\n' | sort) \
      <(echo "$scaled_down_namespaces" | tr ' ' '\n' | sort) \
      | paste -sd, -)
    if [[ -z "$remaining_namespaces" ]]; then
      echo "No other namespaces remain in sync window; deleting the sync window (ID=$window_id)."
      argocd proj windows delete default "$window_id"
    else
      echo "Updating sync window (ID=$window_id) to remove restored namespaces: $remaining_namespaces"
      argocd proj windows update default "$window_id" --namespaces "$remaining_namespaces" --schedule "$schedule" --duration "$duration" --time-zone "$timeZone"
    fi
  else
    echo "No matching ArgoCD sync window found (it may have been already removed)."
  fi

  # Trigger ArgoCD sync for each affected application (assumes app name equals namespace)
  # This will restore original replicas and DaemonSet scheduling from Git manifests.
  for ns in $scaled_down_namespaces; do
    echo "Triggering ArgoCD sync for application/namespace: $ns"
    argocd app sync "$ns" --async --apply-out-of-sync-only
    # Remove the 'nas-dependent=scaled' labels from all workload types in this namespace
    echo "Removing 'nas-dependent=scaled' labels from workloads in namespace: $ns"
    kubectl get deployments -n "$ns" -l nas-dependent=scaled -o name | xargs -r kubectl label -n "$ns" --overwrite nas-dependent-
    kubectl get statefulsets -n "$ns" -l nas-dependent=scaled -o name | xargs -r kubectl label -n "$ns" --overwrite nas-dependent-
    kubectl get daemonsets -n "$ns" -l nas-dependent=scaled -o name | xargs -r kubectl label -n "$ns" --overwrite nas-dependent-
    # Note: DaemonSet nodeSelector patch will be reverted by ArgoCD sync (restoring original spec from Git).
  done

  echo "All workloads have been instructed to restore, and sync window adjustments are complete."
}

# Main script logic: determine "down" or "up" action
if [[ $# -ne 1 ]]; then
  echo "Usage: $0 [down|up]"
  echo "  down: Identify NAS-dependent PVCs, block auto-sync, and scale down dependent workloads."
  echo "  up:   Remove sync window block and trigger ArgoCD to restore original workload state."
  exit 1
fi

action="$1"
case "$action" in
  down)
    check_dependencies
    check_argocd_env_vars
    check_nas_address
    # Perform the scale-down process
    get_pv_list_dependent_on_nas
    get_namespaces_for_pvs
    argocd_login
    create_sync_window
    get_dependent_workloads
    scale_down_workloads
    echo "NAS-dependent workloads have been scaled down. Auto-sync is blocked for their applications."
    ;;
  up)
    check_dependencies
    check_argocd_env_vars
    # Perform the restore process
    argocd_login
    revert_sync_window
    echo "NAS-dependent workloads have been restored to their original state."
    ;;
  *)
    echo "Invalid option: $action. Use 'down' or 'up'."
    exit 1
    ;;
esac
